{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libarary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "#from keras.layers import *\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "#import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import lite\n",
    "#from keras.models import Sequential\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, h5_path):\n",
    "    model.save(h5_path)\n",
    "\n",
    "def load_network(h5_path):\n",
    "    ho_model = tf.keras.models.load_model(h5_path)\n",
    "\n",
    "    return ho_model\n",
    "\n",
    "def convert_to_tflite(model, model_path):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    flat_data = converter.convert()\n",
    "    with open(model_path, 'wb') as f:\n",
    "        f.write(flat_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word={'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_1_2_3():\n",
    "    data_x=[]\n",
    "    data_y=[]\n",
    "    test_x=[]\n",
    "    test_y=[]\n",
    "    cnt=0\n",
    "    cnt1=0\n",
    "    cnt2=0\n",
    "    cnt3=0\n",
    "    for j in range(0,100):\n",
    "        for i in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']:\n",
    "            #print(i+str(j)+\".txt\")\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                f = open(\"matrixWav2\\\\cs\\\\\"+i+\"\\\\\"+i+str(j)+\".txt\")\n",
    "                line = f.readline()\n",
    "                temp=line.split(\",\")\n",
    "                f.close()\n",
    "            except:\n",
    "                cnt+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            temp_np=np.array(list(map(float, temp)))\n",
    "            #temp_np=(temp_np - temp_np.min(axis=0)) / (temp_np.max(axis=0) - temp_np.min(axis=0)) #min-max scaling\n",
    "            temp_np=temp_np.reshape(1248,24,1) #not resize\n",
    "            \n",
    "            \n",
    "            if j in range(80,100):\n",
    "                cnt1+=1\n",
    "                test_x.append(temp_np)\n",
    "                test_y.append(word[i])\n",
    "            \n",
    "            else:\n",
    "                data_x.append(temp_np)\n",
    "                data_y.append(word[i])\n",
    "                \n",
    "    for j in range(0,100):\n",
    "        for i in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']:\n",
    "            #print(i+str(j)+\".txt\")\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                f = open(\"matrixWav2\\\\yj\\\\\"+i+\"\\\\\"+i+str(j)+\".txt\")\n",
    "                line = f.readline()\n",
    "                temp=line.split(\",\")\n",
    "                f.close()\n",
    "            except:\n",
    "                cnt+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            temp_np=np.array(list(map(float, temp)))\n",
    "            #temp_np=(temp_np - temp_np.min(axis=0)) / (temp_np.max(axis=0) - temp_np.min(axis=0)) #min-max scaling\n",
    "            temp_np=temp_np.reshape(1248,24,1) #not resize\n",
    "            \n",
    "            \n",
    "            if j in range(80,100):\n",
    "                cnt2+=1\n",
    "                test_x.append(temp_np)\n",
    "                test_y.append(word[i])\n",
    "            \n",
    "            else:\n",
    "                data_x.append(temp_np)\n",
    "                data_y.append(word[i])            \n",
    "\n",
    "\n",
    "    for j in range(0,100):\n",
    "        for i in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']:\n",
    "            #print(i+str(j)+\".txt\")\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                f = open(\"matrixWav2\\\\yh\\\\\"+i+\"\\\\\"+i+str(j)+\".txt\")\n",
    "                line = f.readline()\n",
    "                temp=line.split(\",\")\n",
    "                f.close()\n",
    "            except:\n",
    "                cnt+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            temp_np=np.array(list(map(float, temp)))\n",
    "            #temp_np=(temp_np - temp_np.min(axis=0)) / (temp_np.max(axis=0) - temp_np.min(axis=0)) #min-max scaling\n",
    "            temp_np=temp_np.reshape(1248,24,1) #not resize\n",
    "            \n",
    "            \n",
    "            if j in range(80,100):\n",
    "                cnt3+=1\n",
    "                test_x.append(temp_np)\n",
    "                test_y.append(word[i])\n",
    "            \n",
    "            else:\n",
    "                data_x.append(temp_np)\n",
    "                data_y.append(word[i])            \n",
    "                \n",
    "    test_x=np.array(test_x)\n",
    "    test_y=np.array(test_y)\n",
    "    data_x=np.array(data_x)\n",
    "    data_y=np.array(data_y)\n",
    "\n",
    "\n",
    "    #type casting\n",
    "\n",
    "\n",
    "    test_y = to_categorical(test_y,num_classes=26)\n",
    "    data_y = to_categorical(data_y,num_classes=26)\n",
    "\n",
    "    return data_x, test_x, data_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_1():\n",
    "    # load user one data\n",
    "    data_x_1=[]\n",
    "    data_y_1=[]\n",
    "    test_x_1=[]\n",
    "    test_y_1=[]\n",
    "\n",
    "    cnt=0\n",
    "    cnt1=0\n",
    "\n",
    "    for j in range(0,100):\n",
    "        for i in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']:\n",
    "            #print(i+str(j)+\".txt\")\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                f = open(\"matrixWav2\\\\cs\\\\\"+i+\"\\\\\"+i+str(j)+\".txt\")\n",
    "                line = f.readline()\n",
    "                temp=line.split(\",\")\n",
    "                f.close()\n",
    "            except:\n",
    "                cnt+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            temp_np=np.array(list(map(float, temp)))\n",
    "            #temp_np=(temp_np - temp_np.min(axis=0)) / (temp_np.max(axis=0) - temp_np.min(axis=0)) #min-max scaling\n",
    "            temp_np=temp_np.reshape(1248,24,1) #not resize\n",
    "            \n",
    "            \n",
    "            if j in range(80,100):\n",
    "                cnt1+=1\n",
    "                test_x_1.append(temp_np)\n",
    "                test_y_1.append(word[i])\n",
    "            \n",
    "            else:\n",
    "                data_x_1.append(temp_np)\n",
    "                data_y_1.append(word[i])            \n",
    "\n",
    "                \n",
    "    test_x_1=np.array(test_x_1)\n",
    "    test_y_1=np.array(test_y_1)\n",
    "    data_x_1=np.array(data_x_1)\n",
    "    data_y_1=np.array(data_y_1)\n",
    "\n",
    "\n",
    "    #type casting\n",
    "\n",
    "\n",
    "    test_y_1 = to_categorical(test_y_1,num_classes=26)\n",
    "    data_y_1 = to_categorical(data_y_1,num_classes=26)\n",
    "\n",
    "    return data_x_1, test_x_1, data_y_1, test_y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_2():\n",
    "    # load user 2 data\n",
    "    data_x_2=[]\n",
    "    data_y_2=[]\n",
    "    test_x_2=[]\n",
    "    test_y_2=[]\n",
    "\n",
    "    cnt=0\n",
    "    cnt2=0\n",
    "\n",
    "    for j in range(0,100):\n",
    "        for i in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']:\n",
    "            #print(i+str(j)+\".txt\")\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                f = open(\"matrixWav2\\\\yj\\\\\"+i+\"\\\\\"+i+str(j)+\".txt\")\n",
    "                line = f.readline()\n",
    "                temp=line.split(\",\")\n",
    "                f.close()\n",
    "            except:\n",
    "                cnt+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            temp_np=np.array(list(map(float, temp)))\n",
    "            #temp_np=(temp_np - temp_np.min(axis=0)) / (temp_np.max(axis=0) - temp_np.min(axis=0)) #min-max scaling\n",
    "            temp_np=temp_np.reshape(1248,24,1) #not resize\n",
    "            \n",
    "            \n",
    "            if j in range(80,100):\n",
    "                cnt2+=1\n",
    "                test_x_2.append(temp_np)\n",
    "                test_y_2.append(word[i])\n",
    "            \n",
    "            else:\n",
    "                data_x_2.append(temp_np)\n",
    "                data_y_2.append(word[i])            \n",
    "\n",
    "                \n",
    "    test_x_2=np.array(test_x_2)\n",
    "    test_y_2=np.array(test_y_2)\n",
    "    data_x_2=np.array(data_x_2)\n",
    "    data_y_2=np.array(data_y_2)\n",
    "\n",
    "\n",
    "    #type casting\n",
    "\n",
    "\n",
    "    test_y_2 = to_categorical(test_y_2,num_classes=26)\n",
    "    data_y_2 = to_categorical(data_y_2,num_classes=26)\n",
    "\n",
    "    return data_x_2, test_x_2, data_y_2, test_y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_3():\n",
    "    # load user 3 data\n",
    "    data_x_2=[]\n",
    "    data_y_2=[]\n",
    "    test_x_2=[]\n",
    "    test_y_2=[]\n",
    "\n",
    "    cnt=0\n",
    "    cnt2=0\n",
    "\n",
    "    for j in range(0,100):\n",
    "        for i in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']:\n",
    "            #print(i+str(j)+\".txt\")\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                f = open(\"matrixWav2\\\\yh\\\\\"+i+\"\\\\\"+i+str(j)+\".txt\")\n",
    "                line = f.readline()\n",
    "                temp=line.split(\",\")\n",
    "                f.close()\n",
    "            except:\n",
    "                cnt+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            temp_np=np.array(list(map(float, temp)))\n",
    "            #temp_np=(temp_np - temp_np.min(axis=0)) / (temp_np.max(axis=0) - temp_np.min(axis=0)) #min-max scaling\n",
    "            temp_np=temp_np.reshape(1248,24,1) #not resize\n",
    "            \n",
    "            \n",
    "            if j in range(80,100):\n",
    "                cnt2+=1\n",
    "                test_x_2.append(temp_np)\n",
    "                test_y_2.append(word[i])\n",
    "            \n",
    "            else:\n",
    "                data_x_2.append(temp_np)\n",
    "                data_y_2.append(word[i])            \n",
    "\n",
    "                \n",
    "    test_x_2=np.array(test_x_2)\n",
    "    test_y_2=np.array(test_y_2)\n",
    "    data_x_2=np.array(data_x_2)\n",
    "    data_y_2=np.array(data_y_2)\n",
    "\n",
    "\n",
    "    #type casting\n",
    "\n",
    "\n",
    "    test_y_2 = to_categorical(test_y_2,num_classes=26)\n",
    "    data_y_2 = to_categorical(data_y_2,num_classes=26)\n",
    "\n",
    "    return data_x_2, test_x_2, data_y_2, test_y_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_m4a_data_1():\n",
    "    data_x=[]\n",
    "    data_y=[]\n",
    "\n",
    "    cnt=0\n",
    "\n",
    "    for j in range(0,100):\n",
    "        for i in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']:\n",
    "            #print(i+str(j)+\".txt\")\n",
    "        \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            try:\n",
    "                f = open(\"matrix\\\\cs\\\\\"+i+\"\\\\\"+i+str(j)+\".txt\")\n",
    "                line = f.readline()\n",
    "                temp=line.split(\",\")\n",
    "                f.close()\n",
    "            except:\n",
    "                cnt+=1\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            temp_np=np.array(list(map(float, temp)))\n",
    "            #temp_np=(temp_np - temp_np.min(axis=0)) / (temp_np.max(axis=0) - temp_np.min(axis=0)) #min-max scaling\n",
    "            temp_np=temp_np.reshape(1248,24,1) #not resize\n",
    "            \n",
    "            \n",
    "            data_x.append(temp_np)\n",
    "            data_y.append(word[i])\n",
    "\n",
    "                \n",
    "\n",
    "    data_x=np.array(data_x)\n",
    "    data_y=np.array(data_y)\n",
    "\n",
    "\n",
    "    #type casting\n",
    "\n",
    "\n",
    "    data_y = to_categorical(data_y,num_classes=26)\n",
    "\n",
    "    return data_x,data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_1:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        pass\n",
    "\n",
    "    def get_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(20, kernel_size = 5, padding=\"same\", input_shape=data_x[0].shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "        model.add(Conv2D(50, kernel_size = 5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(26))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        model.compile(loss = \"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def train_model(self, data_x, data_y):\n",
    "    \n",
    "        self.model.fit(data_x, data_y, epochs=10, batch_size=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_2:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        pass\n",
    "\n",
    "    def init_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(8, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(16, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(32, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(26))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def train_model(self, lr, epoch, batch_size, stop, data_x, data_y):\n",
    "        print(f\"lr : {lr}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0, patience=100, \n",
    "        verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        total_batch = data_x.shape[0]\n",
    "        decay_steps = int((total_batch/batch_size)/4)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            lr, decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0,\n",
    "            name=None\n",
    "        )\n",
    "        opt = optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(loss = \"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "        if stop == True:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2, callbacks=[callback])\n",
    "        else:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "\n",
    "        return history\n",
    "\n",
    "    def search(self, lr, data_x, data_y):\n",
    "\n",
    "        lr_candidate = np.arange(lr[0], lr[1], lr[2])\n",
    "\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for lr in lr_candidate:\n",
    "            history = self.train_model(10**(lr), 1, 32, False, data_x, data_y)\n",
    "            train_acc = history.history[\"accuracy\"]\n",
    "            val_acc = history.history[\"val_accuracy\"]\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_3:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        pass\n",
    "\n",
    "    def init_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "        model.add(Conv2D(256, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(26))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def train_model(self, lr, epoch, batch_size, stop, data_x, data_y):\n",
    "        print(f\"lr : {lr}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0, patience=100, \n",
    "        verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        total_batch = data_x.shape[0]\n",
    "        decay_steps = int((total_batch/batch_size)/4)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            lr, decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0,\n",
    "            name=None\n",
    "        )\n",
    "        opt = optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(loss = \"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "        if stop == True:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2, callbacks=[callback])\n",
    "        else:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "\n",
    "        return history\n",
    "\n",
    "    def search(self, lr, data_x, data_y):\n",
    "\n",
    "        lr_candidate = np.arange(lr[0], lr[1], lr[2])\n",
    "\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for lr in lr_candidate:\n",
    "            history = self.train_model(10**(lr), 1, 32, False, data_x, data_y)\n",
    "            train_acc = history.history[\"accuracy\"]\n",
    "            val_acc = history.history[\"val_accuracy\"]\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_4:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        pass\n",
    "\n",
    "    def init_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(8, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size = (2,2), strides = (2,2)))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(26))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, lr, epoch, batch_size, stop, data_x, data_y):\n",
    "        print(f\"lr : {lr}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0, patience=100, \n",
    "        verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        total_batch = data_x.shape[0]\n",
    "        decay_steps = int((total_batch/batch_size)/4)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            lr, decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0,\n",
    "            name=None\n",
    "        )\n",
    "        opt = optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(loss = \"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "        if stop == True:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2, callbacks=[callback])\n",
    "        else:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "\n",
    "        return history\n",
    "\n",
    "    def search(self, lr, data_x, data_y):\n",
    "\n",
    "        lr_candidate = np.arange(lr[0], lr[1], lr[2])\n",
    "\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for lr in lr_candidate:\n",
    "            history = self.train_model(10**(lr), 1, 32, False, data_x, data_y)\n",
    "            train_acc = history.history[\"accuracy\"]\n",
    "            val_acc = history.history[\"val_accuracy\"]\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_5:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        pass\n",
    "\n",
    "    def init_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(64, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(128, kernel_size = 3, padding=\"same\", strides=(2,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(26))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, lr, epoch, batch_size, stop, data_x, data_y):\n",
    "        print(f\"lr : {lr}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0, patience=100, \n",
    "        verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        total_batch = data_x.shape[0]\n",
    "        decay_steps = int((total_batch/batch_size)/4)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            lr, decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0,\n",
    "            name=None\n",
    "        )\n",
    "        opt = optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(loss = \"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "        if stop == True:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2, callbacks=[callback])\n",
    "        else:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "\n",
    "        return history\n",
    "\n",
    "    def search(self, lr, data_x, data_y):\n",
    "\n",
    "        lr_candidate = np.arange(lr[0], lr[1], lr[2])\n",
    "\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for lr in lr_candidate:\n",
    "            history = self.train_model(10**(lr), 1, 32, False, data_x, data_y)\n",
    "            train_acc = history.history[\"accuracy\"]\n",
    "            val_acc = history.history[\"val_accuracy\"]\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_6:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        pass\n",
    "\n",
    "    def init_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(8, kernel_size = 3, padding=\"same\", strides=(2,2)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(DepthwiseConv2D(kernel_size = 3, padding=\"same\", strides=(1,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size = 1, padding=\"same\", strides=(2,2)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(DepthwiseConv2D(kernel_size = 3, padding=\"same\", strides=(1,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size = 1, padding=\"same\", strides=(2,2)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dense(26))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "        return model\n",
    "    def train_model(self, lr, epoch, batch_size, stop, data_x, data_y):\n",
    "        print(f\"lr : {lr}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0, patience=100, \n",
    "        verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        total_batch = data_x.shape[0]\n",
    "        decay_steps = int((total_batch/batch_size)/4)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            lr, decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0,\n",
    "            name=None\n",
    "        )\n",
    "        opt = optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(loss = \"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "        if stop == True:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2, callbacks=[callback])\n",
    "        else:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "\n",
    "        return history\n",
    "\n",
    "    def search(self, lr, data_x, data_y):\n",
    "\n",
    "        lr_candidate = np.arange(lr[0], lr[1], lr[2])\n",
    "\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for lr in lr_candidate:\n",
    "            history = self.train_model(10**(lr), 1, 32, False, data_x, data_y)\n",
    "            train_acc = history.history[\"accuracy\"]\n",
    "            val_acc = history.history[\"val_accuracy\"]\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_7:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        pass\n",
    "\n",
    "    def init_network(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(8, kernel_size = 3, padding=\"same\", strides=(2,2)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(DepthwiseConv2D(kernel_size = 3, padding=\"same\", strides=(1,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size = 1, padding=\"same\", strides=(2,2)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(DepthwiseConv2D(kernel_size = 3, padding=\"same\", strides=(1,1)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size = 1, padding=\"same\", strides=(2,2)))\n",
    "        model.add(BatchNormalization(axis=3))\n",
    "        model.add(Activation(\"relu\"))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(26))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        self.model = model\n",
    "    \n",
    "        return model\n",
    "\n",
    "    def train_model(self, lr, epoch, batch_size, stop, data_x, data_y):\n",
    "        print(f\"lr : {lr}\")\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=0, patience=100, \n",
    "        verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "        total_batch = data_x.shape[0]\n",
    "        decay_steps = int((total_batch/batch_size)/4)\n",
    "\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "            lr, decay_steps, t_mul=2.0, m_mul=1.0, alpha=0.0,\n",
    "            name=None\n",
    "        )\n",
    "        opt = optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.model.compile(loss = \"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "        if stop == True:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2, callbacks=[callback])\n",
    "        else:\n",
    "            history = self.model.fit(data_x, data_y, epochs=epoch, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "\n",
    "        return history\n",
    "\n",
    "    def search(self, lr, data_x, data_y):\n",
    "\n",
    "        lr_candidate = np.arange(lr[0], lr[1], lr[2])\n",
    "\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for lr in lr_candidate:\n",
    "            history = self.train_model(10**(lr), 1, 32, False, data_x, data_y)\n",
    "            train_acc = history.history[\"accuracy\"]\n",
    "            val_acc = history.history[\"val_accuracy\"]\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "\n",
    "        return train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x1, test_x1, data_y1, test_y1  = load_data_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x2, test_x2, data_y2, test_y2  = load_data_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x3, test_x3, data_y3, test_y3  = load_data_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, test_x, data_y, test_y = load_data_1_2_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=np.concatenate((data_x1,data_x2,data_x3,),axis=0)\n",
    "data_y=np.concatenate((data_y1,data_y2,data_y3),axis=0)\n",
    "test_x=np.concatenate((test_x1,test_x2,test_x3),axis=0)\n",
    "test_y=np.concatenate((test_y1,test_y2,test_y3),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4a_data_x,m4a_data_y=load_m4a_data_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2051, 1248, 24, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4a_data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x=np.concatenate((data_x,m4a_data_x),axis=0)\n",
    "data_y=np.concatenate((data_y,m4a_data_y),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 1248, 24, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531, 1248, 24, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1ddb8ccff48>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net_2()\n",
    "model.init_network()\n",
    "#acc, val_acc = model.search((-3.5, -2.9, 0.1), data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr : 0.01\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DDB68ED9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DDB68ED9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DDB68ED9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "154/154 [==============================] - ETA: 0s - loss: 55.2803 - accuracy: 0.0822WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DDDBAB1288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DDDBAB1288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DDDBAB1288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "154/154 [==============================] - 43s 277ms/step - loss: 55.2803 - accuracy: 0.0822 - val_loss: 3.3386 - val_accuracy: 0.0448\n",
      "Epoch 2/20\n",
      "154/154 [==============================] - 42s 275ms/step - loss: 2.0998 - accuracy: 0.4316 - val_loss: 3.2955 - val_accuracy: 0.1180\n",
      "Epoch 3/20\n",
      "154/154 [==============================] - 42s 273ms/step - loss: 1.7820 - accuracy: 0.5081 - val_loss: 3.5637 - val_accuracy: 0.1115\n",
      "Epoch 4/20\n",
      "154/154 [==============================] - 42s 270ms/step - loss: 1.3768 - accuracy: 0.6358 - val_loss: 2.9847 - val_accuracy: 0.2636\n",
      "Epoch 5/20\n",
      "154/154 [==============================] - 41s 269ms/step - loss: 1.1383 - accuracy: 0.7048 - val_loss: 3.0273 - val_accuracy: 0.3312\n",
      "Epoch 6/20\n",
      "154/154 [==============================] - 42s 271ms/step - loss: 0.9172 - accuracy: 0.7551 - val_loss: 3.2393 - val_accuracy: 0.3002\n",
      "Epoch 7/20\n",
      "154/154 [==============================] - 42s 271ms/step - loss: 0.4438 - accuracy: 0.8815 - val_loss: 3.2748 - val_accuracy: 0.3784\n",
      "Epoch 8/20\n",
      "154/154 [==============================] - 42s 271ms/step - loss: 0.0663 - accuracy: 0.9855 - val_loss: 3.3659 - val_accuracy: 0.4565\n",
      "Epoch 9/20\n",
      "154/154 [==============================] - 42s 275ms/step - loss: 0.0106 - accuracy: 0.9990 - val_loss: 3.4780 - val_accuracy: 0.4744\n",
      "Epoch 10/20\n",
      "154/154 [==============================] - 42s 271ms/step - loss: 0.1030 - accuracy: 0.9719 - val_loss: 4.7787 - val_accuracy: 0.3605\n",
      "Epoch 11/20\n",
      "154/154 [==============================] - 42s 271ms/step - loss: 0.3411 - accuracy: 0.9039 - val_loss: 3.6980 - val_accuracy: 0.4133\n",
      "Epoch 12/20\n",
      "154/154 [==============================] - 42s 272ms/step - loss: 0.0652 - accuracy: 0.9831 - val_loss: 3.9793 - val_accuracy: 0.4296\n",
      "Epoch 13/20\n",
      "154/154 [==============================] - 42s 272ms/step - loss: 0.0259 - accuracy: 0.9941 - val_loss: 4.1494 - val_accuracy: 0.4516\n",
      "Epoch 14/20\n",
      "154/154 [==============================] - 42s 273ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 4.2923 - val_accuracy: 0.4809\n",
      "Epoch 15/20\n",
      "154/154 [==============================] - 42s 272ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 4.7784 - val_accuracy: 0.4866\n",
      "Epoch 16/20\n",
      "154/154 [==============================] - 42s 272ms/step - loss: 3.9243e-04 - accuracy: 1.0000 - val_loss: 4.9626 - val_accuracy: 0.4923\n",
      "Epoch 17/20\n",
      "154/154 [==============================] - 42s 272ms/step - loss: 2.9901e-04 - accuracy: 1.0000 - val_loss: 5.0350 - val_accuracy: 0.4931\n",
      "Epoch 18/20\n",
      "154/154 [==============================] - 42s 272ms/step - loss: 2.3044e-04 - accuracy: 1.0000 - val_loss: 5.0882 - val_accuracy: 0.4890\n",
      "Epoch 19/20\n",
      "154/154 [==============================] - 43s 277ms/step - loss: 2.4686e-04 - accuracy: 1.0000 - val_loss: 5.1010 - val_accuracy: 0.4931\n",
      "Epoch 20/20\n",
      "154/154 [==============================] - 43s 276ms/step - loss: 2.2113e-04 - accuracy: 1.0000 - val_loss: 5.3059 - val_accuracy: 0.4915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ddb8ae45c8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(10**(-2), 20, 32, True, data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DDDB9E4798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DDDB9E4798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DDDB9E4798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "pred=model.model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5961538461538461"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5467015022860875"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8365384615384616"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y1,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User1(m4a) + User1(wav) -> User1(wav) 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173076923076923"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6868686868686869"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y2,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8178294573643411"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y3,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6414108425865448"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.34615384615384 65.45454545454545 45.542635658914726\n"
     ]
    }
   ],
   "source": [
    "result1=0\n",
    "result2=0\n",
    "result3=0\n",
    "\n",
    "cnt1=520\n",
    "cnt2=495\n",
    "cnt3=516\n",
    "\n",
    "for idx,i in enumerate(test_y):\n",
    "    if(np.argmax(i)!=np.argmax(pred[idx])):\n",
    "        if(idx in range(0,cnt1)):\n",
    "            result1+=1\n",
    "        elif(idx in range(cnt1,cnt1+cnt2)):\n",
    "            result2+=1\n",
    "        else:\n",
    "            result3+=1\n",
    "\n",
    "            \n",
    "result1=((cnt1-result1)/cnt1)*100\n",
    "result2=((cnt2-result2)/cnt2)*100\n",
    "result3=((cnt3-result3)/cnt3)*100\n",
    "\n",
    "print(result1,result2,result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.43103448275862066, 'b': 0.4666666666666667, 'c': 0.45614035087719296, 'd': 0.5333333333333333, 'e': 0.43333333333333335, 'f': 0.48333333333333334, 'g': 0.4576271186440678, 'h': 0.7333333333333333, 'i': 0.7931034482758621, 'j': 0.92, 'k': 0.9166666666666666, 'l': 0.55, 'm': 0.7166666666666667, 'n': 0.5833333333333334, 'o': 0.5166666666666667, 'p': 0.7, 'q': 0.6551724137931034, 'r': 0.8166666666666667, 's': 0.47058823529411764, 't': 0.8, 'u': 0.6166666666666667, 'v': 0.5333333333333333, 'w': 0.7666666666666667, 'x': 0.6666666666666666, 'y': 0.9333333333333333, 'z': 0.7333333333333333}\n"
     ]
    }
   ],
   "source": [
    "alpha_result=list([0]*26)\n",
    "alpha_cnt=list([0]*26)\n",
    "for idx,i in enumerate(test_y):\n",
    "    alpha_cnt[np.argmax(i)]+=1\n",
    "    if np.argmax(i)==np.argmax(pred[idx]):\n",
    "        alpha_result[np.argmax(i)]+=1\n",
    "alpha_acc={chr(i):0 for i in range(ord('a'),ord('z')+1)}\n",
    "for idx,i in enumerate(alpha_result):\n",
    "    if(alpha_cnt[idx]!=0):\n",
    "        alpha_acc[chr(idx+ord('a'))]=(alpha_result[idx]/alpha_cnt[idx])\n",
    "print(alpha_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7923076923076923"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6464646464646465"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6975369458128079"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7807692307692308"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6141414141414141"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6975369458128079"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.argmax(test_y,axis=1),np.argmax(pred,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001DDB69E71F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001DDB69E71F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001DDB69E71F8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001DDB6C9E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001DDB6C9E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001DDB6C9E558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\gosan\\AppData\\Local\\Temp\\tmpabjnhiw_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gosan\\AppData\\Local\\Temp\\tmpabjnhiw_\\assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "save_model(model.model, './model/Net2_all.h5')\n",
    "h5_model=load_network('./model/Net2_all.h5')\n",
    "convert_to_tflite(h5_model, './model/Net2_all.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1248, 24, 1) dtype=float32 (created by layer 'conv2d_input')>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 26) dtype=float32 (created by layer 'activation_3')>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001F4AE33B0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001F4AE33B0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x000001F4AE33B0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F4AE4B9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F4AE4B9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000001F4AE4B9E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\gosan\\AppData\\Local\\Temp\\tmpoex5cgwd\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gosan\\AppData\\Local\\Temp\\tmpoex5cgwd\\assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "h5_path='./model/Net2.h5'\n",
    "model_path='./model/Net2.tflite'\n",
    "\n",
    "\n",
    "model.model.save(h5_path)\n",
    "\n",
    "model2 = tf.keras.models.load_model(h5_path)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
    "\n",
    "#converter = lite.TFLiteConverter.from_keras_model_file(h5_path)\n",
    "#tf.lite.TFLiteConverter.from_keras_model_file(h5_path)\n",
    "flat_data = converter.convert()\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    f.write(flat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open(\"a0.txt\")\n",
    "line = f.readline()\n",
    "temp=line.split(\",\")\n",
    "f.close()   \n",
    "\n",
    "temp_np=np.array(list(map(float, temp)))\n",
    "#temp_np=(temp_np - temp_np.min(axis=0)) / (temp_np.max(axis=0) - temp_np.min(axis=0)) #min-max scaling\n",
    "temp_np=temp_np.reshape(1248,24,1) #not resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1248, 24, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
